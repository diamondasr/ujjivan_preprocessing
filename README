Python prprocessing script for Ujjivan dataset

README 

dependencies : FFMpeg, Sox , Python3 , g2p

Steps:

git clone -b v2_lite https://github.com/diamondasr/ujjivan_preprocessing.git ujjivan.preprocessing


cd ujjivan.preprocessing ( use tmux with detached mode)

create new conda environment with python 3.5
Install following packages
{ pip3 install tqdm , pip3 install pandas}


Make sure you have this version of g2p (in home Directory)
git clone https://github.com/diamondasr/g2p.git g2p

Usage:

python3 download.py -lang ta \
 -source_mp3_dir "/home/ubuntu/datasets/ujjivan/voicecollectionblobcontainer/" \
  -destination_wav_dir "/home/ubuntu/datasets/ujjivan/wavs/" \
   -automatic_lexicon_generation True



Directory Structure

destination_wav_dir/{lang_id}
logs/lang_id.*.log

If final_kaldi_format is enabled :
    kaldi_outputs/{lang_id}/{subset_id}/ {data,exp,mfcc, wav.scp,text,spk2utt,utt2spk,lexicon.txt}

If final_kaldi_format is disabled :
    kaldi_outputs/{lang_id}/{subset_id}/ {wav.scp,text,spk2utt,utt2spk,lexicon.txt}
    wav.scp,text,spk2utt,utt2spk,lexicon.txt are files before test split


Working

It downloads transcriptions.json and audio.json from base azure url and provide language code

It then creates a transcriptions.txt file in data/{]ang_id} 
format of transcriptions.txt is <transcription id> <transcription>

For each line in transcriptions.txt , it creates a set of unique words,then writes in to a temp file
the above temp file is passed to g2p script to produce final lexicon file, lexicon.txt


It also stores audio.json in data/{]ang_id} 

It then iterates through each audio file url in audio.json 
    and downloads audio file, converts it to .wav, and also creates/appends to wav.scp,text file,spk2utt for each row

It stores all utterance ids of processed files in a python set, which at the end of execution is written
to data/{lang_id}/lang_id.set and also is read at beginning so same files are never processed again


After all rows are processed, and final wav.scp is obtained, it counts the number of total rows, and
appends it to a file called {lang_id.sets} , and creates a new subset like ta_50, if such split number
already exists, it doesnt do anything.


